{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e82b8cfd-e143-4b95-9184-3386714106bc",
   "metadata": {},
   "source": [
    "### Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "090ea5e5-6fa2-4b9c-9e87-f79df3bb8b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6d8e54e-755b-4ef3-9f68-c5a6c6f01e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos para o dataset e dicionário\n",
    "dataset_path = \"Correto_whatsapp_rotulado_revisado.csv\"\n",
    "dic_path = \"v2_SocialLIWC_formatado_ordenado.dic\"\n",
    "\n",
    "# Carrega o dataset\n",
    "df = pd.read_csv(dataset_path)\n",
    "df['text_content_anonymous'] = df['text_content_anonymous'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893bac1d-5a7d-4892-9e1f-5dc57a3ad7b0",
   "metadata": {},
   "source": [
    "### Carrega o PrejudiceBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73519d99-cc82-4fe4-89a6-939557075f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para carregar o dicionário\n",
    "def carregar_dicionario_personalizado(dic_path):\n",
    "    categorias = {}\n",
    "    lexicon = {}\n",
    "    dentro_das_categorias = False\n",
    "\n",
    "    with open(dic_path, 'r', encoding='utf-8') as file:\n",
    "        for linha in file:\n",
    "            linha = linha.strip()\n",
    "            \n",
    "            # Detecta a seção de categorias delimitada por '%'\n",
    "            if linha == '%':\n",
    "                dentro_das_categorias = not dentro_das_categorias\n",
    "                continue\n",
    "            \n",
    "            # Lê as categorias personalizadas\n",
    "            if dentro_das_categorias:\n",
    "                codigo, categoria = linha.split()\n",
    "                categorias[codigo] = categoria\n",
    "            else:\n",
    "                # Lê as palavras e suas categorias\n",
    "                partes = linha.split(\"\\t\")\n",
    "                palavra = partes[0]\n",
    "                categoria_ids = partes[1:]\n",
    "                lexicon[palavra] = [categorias[codigo] for codigo in categoria_ids if codigo in categorias]\n",
    "    \n",
    "    return lexicon, list(categorias.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5334ec-5b33-471b-b2ad-998670f5b33d",
   "metadata": {},
   "source": [
    "### Tokenizar e organizar as palavras como features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c670442-b9c0-4c47-af74-66c32b17441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenização simples\n",
    "def tokenize(text):\n",
    "    tokens = []\n",
    "    for match in re.finditer(r\"\\w+\", text, re.UNICODE):\n",
    "        tokens.append(match.group(0).lower())\n",
    "    return tokens\n",
    "\n",
    "# Função para incluir palavras do dicionário como colunas\n",
    "def adicionar_palavras_como_features(df, lexicon):\n",
    "    # Criar DataFrame temporário com todas as palavras inicializadas com 0\n",
    "    new_columns = pd.DataFrame(0, index=df.index, columns=list(lexicon.keys()))\n",
    "    \n",
    "    # Concatenar com o DataFrame original de uma vez\n",
    "    df = pd.concat([df, new_columns], axis=1)\n",
    "    \n",
    "    # Contar ocorrências de cada palavra no texto\n",
    "    for i, texto in df['text_content_anonymous'].items():\n",
    "        tokens = tokenize(texto)\n",
    "        for token in tokens:\n",
    "            if token in lexicon:\n",
    "                df.at[i, token] += 1\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11569a81-20fc-4b53-97b6-6fc6b289c2d1",
   "metadata": {},
   "source": [
    "### Configuração do baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fad9060-a3c9-4bca-aa7f-9d06c62cd2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o dicionário\n",
    "lexicon, category_names = carregar_dicionario_personalizado(dic_path)\n",
    "\n",
    "# Adiciona palavras como features\n",
    "df = adicionar_palavras_como_features(df, lexicon)\n",
    "\n",
    "# Define as colunas de entrada (todas as palavras do dicionário) e o rótulo\n",
    "X = df[list(lexicon.keys())]\n",
    "y = df['preconceito']\n",
    "total_features = X.shape[1] \n",
    "\n",
    "# Divisão dos dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef44ad6e-6a40-433d-be90-33e8dfbec330",
   "metadata": {},
   "source": [
    "### Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c471c02-6007-4ca9-b930-447acce12284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos\n",
    "models = [\n",
    "    LogisticRegression(), BernoulliNB(), MultinomialNB(), LinearSVC(dual=False),\n",
    "    KNeighborsClassifier(), RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(n_estimators=200), MLPClassifier(batch_size=64, max_iter=50, early_stopping=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a4702b-fb54-4825-8f98-51c5b0c15d6f",
   "metadata": {},
   "source": [
    "### Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "014a3e11-f304-4783-b6c9-0c3925f67730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação de modelos com validação cruzada e teste final\n",
    "resultados = []\n",
    "\n",
    "for model in models:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Validação Cruzada (métricas médias dos folds de validação)\n",
    "    metodos_scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    cv_results = cross_validate(model, X_train, y_train, cv=5, scoring=metodos_scoring, return_train_score=True)\n",
    "    \n",
    "    # Treina o modelo no conjunto de treino completo\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Prevê no conjunto de teste (holdout)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Métricas no teste\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_precision = precision_score(y_test, y_pred)\n",
    "    test_recall = recall_score(y_test, y_pred)\n",
    "    test_f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Calcula ROC AUC no teste (manuseia modelos sem predict_proba)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_proba = model.decision_function(X_test)\n",
    "    test_roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Compila resultados\n",
    "    resultado = {\n",
    "        'model': model.__class__.__name__,\n",
    "        'execution_time': end_time - start_time,\n",
    "        'total_features': total_features,\n",
    "    }\n",
    "    \n",
    "    # Métricas de validação cruzada (média dos folds)\n",
    "    for scoring in metodos_scoring:\n",
    "        resultado[f'cv_{scoring}_avg'] = cv_results[f'test_{scoring}'].mean()\n",
    "        resultado[f'cv_{scoring}_std'] = cv_results[f'test_{scoring}'].std()\n",
    "    \n",
    "    # Métricas no teste (holdout)\n",
    "    resultado.update({\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall,\n",
    "        'test_f1': test_f1,\n",
    "        'test_roc_auc': test_roc_auc\n",
    "    })\n",
    "    \n",
    "    resultados.append(resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43111c64-6bc9-4580-abdb-47af77e7b25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        model  total_features  execution_time  \\\n",
      "0          LogisticRegression             842           1.094   \n",
      "1                 BernoulliNB             842           0.894   \n",
      "2               MultinomialNB             842           0.587   \n",
      "3                   LinearSVC             842           0.675   \n",
      "4        KNeighborsClassifier             842           3.567   \n",
      "5      RandomForestClassifier             842           4.685   \n",
      "6  GradientBoostingClassifier             842           8.213   \n",
      "7               MLPClassifier             842          15.034   \n",
      "\n",
      "   cv_accuracy_avg  test_accuracy  cv_precision_avg  test_precision  \\\n",
      "0            0.828          0.828             0.930           0.934   \n",
      "1            0.826          0.813             0.892           0.873   \n",
      "2            0.827          0.815             0.884           0.871   \n",
      "3            0.832          0.815             0.920           0.909   \n",
      "4            0.770          0.797             0.840           0.897   \n",
      "5            0.823          0.805             0.905           0.910   \n",
      "6            0.825          0.817             0.943           0.936   \n",
      "7            0.830          0.820             0.917           0.910   \n",
      "\n",
      "   cv_recall_avg  test_recall  cv_f1_avg  test_f1  cv_roc_auc_avg  \\\n",
      "0          0.711        0.707      0.805    0.805           0.900   \n",
      "1          0.742        0.733      0.810    0.797           0.899   \n",
      "2          0.753        0.740      0.812    0.800           0.897   \n",
      "3          0.728        0.700      0.812    0.791           0.901   \n",
      "4          0.704        0.670      0.754    0.767           0.860   \n",
      "5          0.722        0.677      0.803    0.776           0.895   \n",
      "6          0.693        0.680      0.798    0.788           0.898   \n",
      "7          0.727        0.710      0.810    0.798           0.898   \n",
      "\n",
      "   test_roc_auc  \n",
      "0         0.902  \n",
      "1         0.903  \n",
      "2         0.900  \n",
      "3         0.896  \n",
      "4         0.836  \n",
      "5         0.898  \n",
      "6         0.900  \n",
      "7         0.900  \n"
     ]
    }
   ],
   "source": [
    "# Exibe resultados formatados\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "cols_order = [\n",
    "    'model', 'total_features', 'execution_time',\n",
    "    'cv_accuracy_avg', 'test_accuracy',\n",
    "    'cv_precision_avg', 'test_precision',\n",
    "    'cv_recall_avg', 'test_recall',\n",
    "    'cv_f1_avg', 'test_f1',\n",
    "    'cv_roc_auc_avg', 'test_roc_auc'\n",
    "]\n",
    "\n",
    "print(df_resultados[cols_order].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68374406-284b-482c-a74f-9a23ab71586f",
   "metadata": {},
   "source": [
    "### Salva os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9877fbe-47ba-4971-a119-9b572009f895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas salvas em: wpp_dic_model_performance_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Salva as métricas no CSV\n",
    "metrics_output_path = \"wpp_dic_model_performance_metrics.csv\"\n",
    "df_resultados.to_csv(metrics_output_path, index=False)\n",
    "print(f\"Métricas salvas em: {metrics_output_path}\")\n",
    "\n",
    "# Salva o dataset processado\n",
    "df.to_csv(\"wpp_processado.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
